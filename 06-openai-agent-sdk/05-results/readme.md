# **Results:**

When you invoke an agent via the SDK—using `Runner.run()`, `run_sync()`, or `run_streamed()`—you receive a **RunResult** (or a `RunResultStreaming` for streamed calls). Both inherit from **RunResultBase**, which holds all the useful post-run data you need.

```python
import os
from agents import Agent, Runner, function_tool, OpenAIChatCompletionsModel, RunConfig
from pydantic import BaseModel

# ———————————————————————————————— Setup ————————————————————————————————
api_key = os.getenv("GEMINI_API_KEY")
client = AsyncOpenAI(api_key=GEMINI_API_KEY, base_url="https://generativelanguage.googleapis.com/v1beta/openai/")
model = OpenAIChatCompletionsModel(model="gemini-2.0-flash", openai_client=client)

# A simple tool to demo tool calls in the result
@function_tool
def add(a: int, b: int) -> int:
    return a + b

# Create an agent that may call our add tool
agent = Agent(
    name="AdderAgent",
    instructions="Use the add tool to sum two numbers and return a JSON with the field `result`.",
    model=model,
)

# ———————————————————————————————— Run ————————————————————————————————
# You can also pass RunConfig here for run-level overrides
run_result = Runner.run_sync(agent, "Please add 7 and 5.")

# ———————————————————————————————— Inspecting RunResult ————————————————————————————————

# Results  (the full RunResult object)
print("Full RunResult:")
print(run_result)
```

## 1. **Final output:**

- **What it is**: The ultimate answer produced by the last agent in your workflow.
- **Type**:
  - A plain `str` if the agent had no structured `output_type`.
  - An instance of the Pydantic model you defined under `output_type`, if you set one.
- **When to use**: Present this directly to end users, save it to your database, or feed into downstream logic.
- **Minimal Example:**
  ```python
  # Final output
  #   As we set output_type=SumResult, final_output is a SumResult instance
  print("Final output:")
  print(run_result.final_output)          # SumResult(result=12)
  print("  As dict:", run_result.final_output.dict())
  ```

## 2. **Inputs for the next turn:.**

- **What it is**: A concatenation of your original prompt plus every message or tool result generated during the run.
- **How to get it**: Call `result.to_input_list()`.
- **Why it matters**: If you’re building a multi-turn chat or chaining agents in a loop, this list becomes the “chat history” or “context” for the next invocation.
- **Minimal Example:**
  ```python
  # Inputs for the next turn
  #   This includes the original user prompt and all LLM/tool messages
  print("Inputs for next turn:")
  for msg in run_result.to_input_list():
      print(f" - {msg.role}: {msg.content}")
  ```

## 3. **Last agent:**

The [`last_agent`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.last_agent) property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.

- **What it is**: The specific `Agent` instance that finished last—especially relevant if you used **handoffs** between multiple agents.
- **Why it matters**: In a multi-agent system (e.g. a triage agent handing off to a booking agent), you’ll often want to resume the same agent next time rather than restarting the entire chain[.](https://openai.github.io/openai-agents-python/results/)
- **Minimal Example:**
  ```python
  # Last agent
  #   If you had handoffs, this tells you which agent finished last
  print("Last agent:")
  print(run_result.last_agent.name)
  ```

## 4. **New items:**

The [`new_items`](https://openai.github.io/openai-agents-python/ref/result/#agents.result.RunResultBase.new_items) property contains the new items generated during the run. The items are [`RunItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.RunItem)s. A run item wraps the raw item generated by the LLM.

- [`MessageOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.MessageOutputItem) indicates a message from the LLM. The raw item is the message generated.
- [`HandoffCallItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.HandoffCallItem) indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
- [`HandoffOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.HandoffOutputItem) indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
- [`ToolCallItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ToolCallItem) indicates that the LLM invoked a tool.
- [`ToolCallOutputItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ToolCallOutputItem) indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
- [`ReasoningItem`](https://openai.github.io/openai-agents-python/ref/items/#agents.items.ReasoningItem) indicates a reasoning item from the LLM. The raw item is the reasoning generated.
- **Minimal Example:**
  ```python
  # New items
  #   A list of RunItem objects created during this run
  print("New items:")
  for item in run_result.new_items:
      print(f" - {item.__class__.__name__}: {item}")
  ```

## 5. **Other information:**

### 1. **Guardrail results** (`input_guardrail_results`, `output_guardrail_results`):

- **What they are**: Any checks you defined to validate or sanitize content before/after the LLM runs.
- **Why it matters**: You can log policy violations (e.g. disallowed content) or enforce data-quality checks before you trust an agent’s answer.
- **Minimal Example:**
  ```python
  # Guardrail results
  print("Guardrail results:")
  print("  Input guardrails:", run_result.input_guardrail_results)
  print("  Output guardrails:", run_result.output_guardrail_results)
  ```

### 2. **Raw responses** (`raw_responses`)**:**

- **What they are**: The low-level `ModelResponse` objects returned by your LLM provider—useful for debugging or audit logs.
- **Why it matters**: You can log policy violations (e.g. disallowed content) or enforce data-quality checks before you trust an agent’s answer.
- **Minimal Example:**
  ```python
  # Raw responses
  #   Low-level LLM provider responses, including usage metadata and chunks
  print("Raw responses:")
  for raw in run_result.raw_responses:
      print(f" - {raw}")
  ```

### 3. **Original input** (`input`):

- **What they are**: The exact prompt or data you first passed into `run()`. Usually you know this already, but it’s here if you need it.
- **Why it matters**: You can log policy violations (e.g. disallowed content) or enforce data-quality checks before you trust an agent’s answer.
- **Minimal Example:**
  ```python
  # Original input
  print("Original input:")
  print(run_result.input)
  ```

<!-- Function Calling -->
<!-- Hosted Tools -->
<!-- LangChain Tools -->
<!-- CrewAI Tools -->
